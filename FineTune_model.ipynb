{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyvpLsqb346FeA5+q2pkXk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ArYdSjoEOUTp"},"outputs":[],"source":["def FineTune_model(beauty_qa_standardizedV2, model, tokenizer, max_seq_length):\n","  '''\n","  This function fine-tunes a small language model to behave like a chatbot using efficient training techniques that reduce memory use and training time.\n","  It uses a 4-bit quantized model and LoRA adapters to update only a subset of the modelâ€™s parameters and includes memory tracking for analysis\n","  '''\n","\n","  # Peft(Parameter efficient fine-tuning tried here; update a small subset of model parameters)\n","  # peft matches performance of full fine-tuning,\n","  # sometimes might lead to better generalization as state-of-the-art models are massively over-parametrized\n","  # Soon, we might hit global compute capacity. So, perform minimum computing possible\n","  # This wraps the original model with fine-tuning adapters.\n","  model = FastLanguageModel.get_peft_model(\n","      model,\n","      r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","      target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n","      lora_alpha = 16,\n","      lora_dropout = 0, # Supports any, but = 0 is optimized\n","      bias = \"none\",    # Supports any, but = \"none\" is optimized\n","      # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","      use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","      random_state = 3407,\n","      use_rslora = False,  # We support rank stabilized LoRA\n","      loftq_config = None, # And LoftQ\n","  )\n","\n","    # @title Fine tune model(SFT), to make it like a chatbot\n","  trainer = SFTTrainer(\n","      model = model,\n","      tokenizer = tokenizer,\n","      train_dataset = beauty_qa_standardizedV2,\n","      dataset_text_field = \"text\",\n","      max_seq_length = max_seq_length,\n","      data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n","      dataset_num_proc = 2,\n","      packing = False, # Can make training 5x faster for short sequences.\n","      args = SFTConfig(\n","          per_device_train_batch_size = 2,\n","          gradient_accumulation_steps = 4,\n","          warmup_steps = 5,\n","          # num_train_epochs = 1, # Set this for 1 full training run.\n","          max_steps = 10,\n","          learning_rate = 2e-4,\n","          logging_steps = 1,\n","          optim = \"adamw_8bit\",\n","          weight_decay = 0.01,\n","          lr_scheduler_type = \"linear\",\n","          seed = 3407,\n","          output_dir = \"outputs\",\n","          report_to = \"none\", # Use this for WandB etc\n","      ),\n","  )\n","  trainer = train_on_responses_only(\n","      trainer,\n","      instruction_part = \"<|im_start|>user\\n\",\n","      response_part = \"<|im_start|>assistant\\n\",\n","  )\n","\n","  tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n","\n","  space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n","  tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n","\n","  # @title Show current memory stats\n","  gpu_stats = torch.cuda.get_device_properties(0)\n","  start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","  max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","  print(\"Before fine-tuning:\\n\")\n","  print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","  print(f\"{start_gpu_memory} GB of memory reserved.\")\n","\n","  trainer_stats = trainer.train()\n","  # The training step modifies the model weights in-place (specifically, the LoRA adapter weights).\n","  # No separate model loading is done for inference\n","\n","\n","  # @title Show final memory and time stats\n","  used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","  used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","  used_percentage = round(used_memory / max_memory * 100, 3)\n","  lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","  print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","  print(\n","      f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n","  )\n","  print(f\"Peak reserved memory = {used_memory} GB.\")\n","  print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","  print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","  print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n","  return model"]}]}