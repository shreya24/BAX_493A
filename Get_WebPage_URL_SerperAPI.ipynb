{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBG4DeZXeVH1pdaBzq13AM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vjU_jCOoMGaw"},"outputs":[],"source":["# Create a session with retry configuration\n","def create_session():\n","    session = requests.Session()\n","\n","    # Configure retry strategy with exponential backoff\n","    retries = Retry(\n","        total=5,  # Total number of retries\n","        backoff_factor=1,  # Exponential backoff factor\n","        status_forcelist=[429, 500, 502, 503, 504],  # Status codes to retry on\n","        allowed_methods=[\"POST\", \"GET\"],  # HTTP methods to retry\n","    )\n","\n","    # Apply retry strategy to both http and https\n","    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)\n","    session.mount(\"http://\", adapter)\n","    session.mount(\"https://\", adapter)\n","\n","    return session\n","\n","def search_serper(query, api_key, retries=3):\n","    \"\"\"\n","    Search using Serper API for the specific query\n","    search_type can be \"search\" or \"maps\"\n","    \"\"\"\n","    # Determine the endpoint based on the search type\n","    endpoint = \"https://google.serper.dev/search\"\n","\n","    headers = {\n","        \"X-API-KEY\": api_key,\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","    search_params = {\n","        \"q\": query,\n","        \"gl\": \"us\",\n","        \"hl\": \"en\"\n","    }\n","\n","    # we want to limit results\n","    search_params[\"num\"] = 6\n","\n","    # Create a dedicated session for this request\n","    session = create_session()\n","\n","    try:\n","        # Add randomized delay to avoid hitting rate limits and predictable patterns\n","        jitter = random.uniform(MIN_REQUEST_DELAY, MAX_REQUEST_DELAY)\n","        time.sleep(jitter)\n","\n","        response = session.post(endpoint, headers=headers, json=search_params, timeout=(10, 30))\n","        response.raise_for_status()\n","        # print(response.json())\n","        return response.json()\n","\n","    except Exception as e:\n","        print(f\"Error searching for '{query}': {e}\")\n","        # If we still have retries left and it's a connection error, try again with longer delay\n","        if retries > 0 and (\"Connection\" in str(e) or \"Timeout\" in str(e) or \"Max retries\" in str(e)):\n","            print(f\"Retrying search for '{query}' ({retries} retries left)\")\n","            # Exponential backoff\n","            time.sleep(2 ** (4 - retries) + random.uniform(1, 3))\n","            return search_serper(query, api_key, retries - 1)\n","        return None\n","    finally:\n","        # Close the session to release resources\n","        session.close()\n","\n","def process_agency(product_name):\n","    \"\"\"\n","    Process a single product name:\n","    Search for \"product name ingredients list\"\n","    \"\"\"\n","    try:\n","        # Search for product name + \"ingredients list\"\n","        website_query = f\"{product_name}\"\n","        website_results = search_serper(website_query, API_KEY)\n","        return website_results\n","\n","    except Exception as e:\n","        print(f\"Error processing {product_name}: {e}\")\n","\n","\n","def get_webpage_url(user_query):\n","    # website_results = process_agency(user_query)\n","    website_results = process_agency(user_query)\n","    if \"organic\" in website_results and len(website_results[\"organic\"]) > 0:\n","        for each_result in website_results[\"organic\"]:\n","            if \"incidecoder.com\" in each_result.get(\"link\", \"\") :\n","                return each_result.get(\"link\", \"\")\n","        return website_results[\"organic\"][0].get(\"link\", \"\")\n","    return \"\""]}]}