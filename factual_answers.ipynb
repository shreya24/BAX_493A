{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMn4CCu6T103QCRMpf8CSaM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oOgVtzLeJItD"},"outputs":[],"source":["def factual_answers(prompt, collection):\n","    query_embedding = embedding_model.encode(prompt)\n","    doc_results = collection.query(query_embeddings=query_embedding, n_results=5)\n","\n","    if not doc_results or not doc_results.get(\"documents\") or not doc_results['documents'][0]:\n","        print(\"No documents found.\")\n","        return None, None\n","\n","    full_context = \"\\n\".join(doc_results['documents'][0])\n","    augmented_prompt = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in answering questions about harmful side-effects of ingredients in cosmetics. Context: \" + full_context},\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","\n","    input_ids = tokenizer.apply_chat_template(\n","        augmented_prompt,\n","        tokenize=True,\n","        add_generation_prompt=True,\n","        return_tensors=\"pt\",\n","    ).to(\"cuda\")\n","\n","    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n","    print(\"Generating output:..\")\n","\n","    output = model.generate(\n","        input_ids,\n","        streamer=text_streamer,\n","        max_new_tokens=128,\n","        pad_token_id=tokenizer.eos_token_id,\n","        use_cache=True,\n","        temperature=0.1,\n","        min_p=0.1\n","    )\n","\n","    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True).split(\"assistant\\n\")[-1].strip()\n","    return decoded_output, doc_results['documents'][0]  # return both response and context\n","\n","\n","  # The answers are pretty accurate.\n","  # I got this error many times:\n","  # Unsloth: Input IDs of length 3023 > the model's max sequence length of 2048.\n","  # We shall truncate it ourselves. It's imperative if you correct this issue first.\n","  # This is happening because since our contexts are long, it is exceeding maximum tokens allowed in context window"]},{"cell_type":"code","source":["def generate_review_loop(prompt, collection, review_model, review_tokenizer, max_attempts=3):\n","    from transformers import TextStreamer\n","    import torch\n","\n","    def factual_answers(prompt, collection):\n","        query_embedding = embedding_model.encode(prompt)\n","        doc_results = collection.query(query_embeddings=query_embedding, n_results=5)\n","\n","        if not doc_results or not doc_results.get(\"documents\") or not doc_results['documents'][0]:\n","            print(\"No documents found.\")\n","            return None, None\n","\n","        full_context = \"\\n\".join(doc_results['documents'][0])\n","        augmented_prompt = [\n","            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in answering questions about harmful side-effects of ingredients in cosmetics. Context: \" + full_context},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","\n","        input_ids = tokenizer.apply_chat_template(\n","            augmented_prompt,\n","            tokenize=True,\n","            add_generation_prompt=True,\n","            return_tensors=\"pt\",\n","        ).to(\"cuda\")\n","\n","        text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n","        print(\"Generating output...\")\n","\n","        output = model.generate(\n","            input_ids,\n","            streamer=text_streamer,\n","            max_new_tokens=128,\n","            pad_token_id=tokenizer.eos_token_id,\n","            use_cache=True,\n","            temperature=0.1,\n","            min_p=0.1\n","        )\n","\n","        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True).split(\"assistant\\n\")[-1].strip()\n","        return decoded_output, doc_results['documents'][0]\n","\n","    def reviewer_agent(query, response, context, review_model, review_tokenizer):\n","        critique_prompt = f\"\"\"You are a quality reviewer.\n","\n","Original Question: {query}\n","\n","Retrieved Context:\n","{context}\n","\n","Generated Response:\n","{response}\n","\n","Evaluate the answer based on:\n","- Whether it correctly and completely answers the query\n","- Whether it is factually grounded in the provided context\n","- Clarity, specificity, and accuracy\n","\n","Respond with:\n","Decision: APPROVE or REJECT\n","Reason: (Explain your reasoning briefly)\n","\"\"\"\n","\n","        inputs = review_tokenizer(critique_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n","        output = review_model.generate(\n","            **inputs,\n","            max_new_tokens=256,\n","            temperature=0.1\n","        )\n","        decoded = review_tokenizer.decode(output[0], skip_special_tokens=True)\n","        return decoded\n","\n","    def parse_reviewer_output(review_output):\n","        lines = review_output.strip().splitlines()\n","        decision_line = next((line for line in lines if \"DECISION\" in line.upper()), \"\")\n","        return \"APPROVE\" in decision_line.upper()\n","\n","    print(f\"\\nUser Query: {prompt}\")\n","    print(\"=\" * 60)\n","\n","    for attempt in range(1, max_attempts + 1):\n","        print(f\"\\nAttempt {attempt}\")\n","        print(\"-\" * 40)\n","\n","        response, context = factual_answers(prompt, collection)\n","        if not response:\n","            print(\"✗ No response generated.\")\n","            continue\n","\n","        print(f\"Generated Response:\\n{response}\\n\")\n","\n","        review_output = reviewer_agent(prompt, response, context, review_model, review_tokenizer)\n","        print(f\"Reviewer Output:\\n{review_output}\\n\")\n","\n","        if parse_reviewer_output(review_output):\n","            print(\"✓ APPROVED by Reviewer\")\n","            return response\n","\n","        print(\"✗ REJECTED - Improving...\")\n","\n","    print(\"Max attempts reached. Returning last response.\")\n","    return response"],"metadata":{"id":"jhJcxKtfBCaz"},"execution_count":null,"outputs":[]}]}